{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raf4el12/consultas/blob/main/BD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map:"
      ],
      "metadata": {
        "id": "137hfSgttxfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKHWjrDmnbEc",
        "outputId": "dfc58d45-8b48-4837-d6fa-7482fd766ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "empleados = [\n",
        "    (\"Ana\", \"Ventas\", 48000),\n",
        "    (\"Carlos\", \"Marketing\", 54000),\n",
        "    (\"Luis\", \"Finanzas\", 60000),\n",
        "    (\"Marta\", \"Ventas\", 52000),\n",
        "    (\"Sofía\", \"Finanzas\", 58000)\n",
        "]\n",
        "\n",
        "rdd_empleados = sc.parallelize(empleados)\n",
        "\n",
        "rdd_empleados_mensual = rdd_empleados.map(lambda x: (x[0], x[1], x[2], x[2] / 12))\n",
        "\n",
        "resultado = rdd_empleados_mensual.collect()\n",
        "print(\"Empleados con salario mensual:\")\n",
        "for empleado in resultado:\n",
        "    print(empleado)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqDK30NItlI3",
        "outputId": "e01a6a29-1c14-445d-db86-a720b4897256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empleados con salario mensual:\n",
            "('Ana', 'Ventas', 48000, 4000.0)\n",
            "('Carlos', 'Marketing', 54000, 4500.0)\n",
            "('Luis', 'Finanzas', 60000, 5000.0)\n",
            "('Marta', 'Ventas', 52000, 4333.333333333333)\n",
            "('Sofía', 'Finanzas', 58000, 4833.333333333333)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter:\n"
      ],
      "metadata": {
        "id": "_vbUz3K0t_Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "empleados = [\n",
        "    (\"Ana\", \"Ventas\", 48000),\n",
        "    (\"Carlos\", \"Marketing\", 54000),\n",
        "    (\"Luis\", \"Finanzas\", 60000),\n",
        "    (\"Marta\", \"Ventas\", 52000),\n",
        "    (\"Sofía\", \"Finanzas\", 58000),\n",
        "    (\"Juan\", \"IT\", 45000)\n",
        "]\n",
        "\n",
        "rdd_empleados = sc.parallelize(empleados)\n",
        "\n",
        "rdd_empleados_filtrados = rdd_empleados.filter(lambda x: x[2] > 50000)\n",
        "\n",
        "resultado = rdd_empleados_filtrados.collect()\n",
        "print(\"Empleados con salario anual superior a 50,000:\")\n",
        "for empleado in resultado:\n",
        "    print(empleado)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xQSFq9nuZEh",
        "outputId": "07bf3049-0027-4b9a-b0a8-dd34797ef651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empleados con salario anual superior a 50,000:\n",
            "('Carlos', 'Marketing', 54000)\n",
            "('Luis', 'Finanzas', 60000)\n",
            "('Marta', 'Ventas', 52000)\n",
            "('Sofía', 'Finanzas', 58000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FlatMap"
      ],
      "metadata": {
        "id": "HAzkz2evufyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "oraciones = [\n",
        "    \"Aprendiendo todos los dias\",\n",
        "    \"Spark es una herramienta poderosa para big data\",\n",
        "    \"Navidad esta cada vez mas cerca\"\n",
        "]\n",
        "\n",
        "rdd_oraciones = sc.parallelize(oraciones)\n",
        "\n",
        "rdd_palabras = rdd_oraciones.flatMap(lambda oracion: oracion.split(\" \"))\n",
        "\n",
        "resultado = rdd_palabras.collect()\n",
        "print(\"Lista de palabras en las oraciones:\")\n",
        "for palabra in resultado:\n",
        "    print(palabra)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW6_ob9bufBk",
        "outputId": "c4d88160-4af7-4197-c5f8-4aa41d384e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lista de palabras en las oraciones:\n",
            "Aprendiendo\n",
            "todos\n",
            "los\n",
            "dias\n",
            "Spark\n",
            "es\n",
            "una\n",
            "herramienta\n",
            "poderosa\n",
            "para\n",
            "big\n",
            "data\n",
            "Navidad\n",
            "esta\n",
            "cada\n",
            "vez\n",
            "ams\n",
            "cerca\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Union:"
      ],
      "metadata": {
        "id": "lWaE8Mk7wcZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "curso_a = [(\"Ana\", \"Matemáticas\"), (\"Luis\", \"Historia\"), (\"Sofía\", \"Química\")]\n",
        "curso_b = [(\"Carlos\", \"Matemáticas\"), (\"Marta\", \"Física\"), (\"Ana\", \"Biología\")]\n",
        "\n",
        "rdd_curso_a = sc.parallelize(curso_a)\n",
        "rdd_curso_b = sc.parallelize(curso_b)\n",
        "\n",
        "rdd_union = rdd_curso_a.union(rdd_curso_b)\n",
        "\n",
        "resultado = rdd_union.collect()\n",
        "print(\"Lista combinada de estudiantes de ambos cursos:\")\n",
        "for estudiante in resultado:\n",
        "    print(estudiante)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwnDSR5Gwe8q",
        "outputId": "0c31d10f-9812-418b-d19e-2d5a66c081d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lista combinada de estudiantes de ambos cursos:\n",
            "('Ana', 'Matemáticas')\n",
            "('Luis', 'Historia')\n",
            "('Sofía', 'Química')\n",
            "('Carlos', 'Matemáticas')\n",
            "('Marta', 'Física')\n",
            "('Ana', 'Biología')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intersection:"
      ],
      "metadata": {
        "id": "khazX0J9wjW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "tienda_a = [(\"Laptop\", 1200), (\"Tablet\", 300), (\"Celular\", 500)]\n",
        "tienda_b = [(\"Celular\", 500), (\"Televisión\", 800), (\"Tablet\", 300)]\n",
        "\n",
        "rdd_tienda_a = sc.parallelize(tienda_a)\n",
        "rdd_tienda_b = sc.parallelize(tienda_b)\n",
        "\n",
        "rdd_interseccion = rdd_tienda_a.intersection(rdd_tienda_b)\n",
        "\n",
        "resultado = rdd_interseccion.collect()\n",
        "print(\"Productos comunes en ambas tiendas:\")\n",
        "for producto in resultado:\n",
        "    print(producto)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2dtWNUcwpgP",
        "outputId": "3276065c-3322-448f-bc53-23ceec30a5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Productos comunes en ambas tiendas:\n",
            "('Tablet', 300)\n",
            "('Celular', 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distinc:"
      ],
      "metadata": {
        "id": "0tZUdlxQwthZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "productos = [\n",
        "    \"Laptop\", \"Tablet\", \"Sofá\", \"Celular\", \"Tablet\", \"Celular\", \"Laptop\", \"Cámara\"\n",
        "]\n",
        "\n",
        "rdd_productos = sc.parallelize(productos)\n",
        "\n",
        "rdd_unicos = rdd_productos.distinct()\n",
        "\n",
        "resultado = rdd_unicos.collect()\n",
        "print(\"Lista de productos únicos:\")\n",
        "for producto in resultado:\n",
        "    print(producto)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZyNl6RVwxaY",
        "outputId": "85d73d59-3063-4ba2-9b9e-e0517b61d63a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lista de productos únicos:\n",
            "Tablet\n",
            "Sofá\n",
            "Celular\n",
            "Laptop\n",
            "Cámara\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GroupByKey"
      ],
      "metadata": {
        "id": "CmqvUZbow15v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "ventas = [\n",
        "    (\"Electrónica\", 300),\n",
        "    (\"Electrónica\", 450),\n",
        "    (\"Ropa\", 120),\n",
        "    (\"Ropa\", 80),\n",
        "    (\"Hogar\", 150),\n",
        "    (\"Electrónica\", 250),\n",
        "    (\"Hogar\", 200)\n",
        "]\n",
        "\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "\n",
        "rdd_ventas_agrupadas = rdd_ventas.groupByKey()\n",
        "\n",
        "resultado = rdd_ventas_agrupadas.collect()\n",
        "print(\"Montos de ventas agrupados por departamento:\")\n",
        "for departamento, montos in resultado:\n",
        "    print(departamento, list(montos))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUIVvRGNw6cT",
        "outputId": "5edb8d06-8124-4cc7-aec8-e0d5e7f89e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Montos de ventas agrupados por departamento:\n",
            "Electrónica [300, 450, 250]\n",
            "Hogar [150, 200]\n",
            "Ropa [120, 80]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reduceByKey"
      ],
      "metadata": {
        "id": "qKYfNCfwxBGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "ventas = [\n",
        "    (\"Electrónica\", 300),\n",
        "    (\"Electrónica\", 450),\n",
        "    (\"Ropa\", 120),\n",
        "    (\"Ropa\", 80),\n",
        "    (\"Hogar\", 150),\n",
        "    (\"Electrónica\", 250),\n",
        "    (\"Hogar\", 200)\n",
        "]\n",
        "\n",
        "rdd_ventas = sc.parallelize(ventas)\n",
        "\n",
        "rdd_totales = rdd_ventas.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "resultado = rdd_totales.collect()\n",
        "print(\"Total de ventas por departamento:\")\n",
        "for departamento, total in resultado:\n",
        "    print(departamento, total)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBF_7tj_xE6U",
        "outputId": "ed8702ea-e5de-481c-d015-9b31b2cdd018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de ventas por departamento:\n",
            "Electrónica 1000\n",
            "Hogar 350\n",
            "Ropa 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SortByKey"
      ],
      "metadata": {
        "id": "axyUnwwRxLxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "empleados = [\n",
        "    (48000, \"Ana\"),\n",
        "    (54000, \"Carlos\"),\n",
        "    (60000, \"Luis\"),\n",
        "    (52000, \"Marta\"),\n",
        "    (58000, \"Sofía\"),\n",
        "    (45000, \"Juan\")\n",
        "]\n",
        "\n",
        "rdd_empleados = sc.parallelize(empleados)\n",
        "\n",
        "rdd_ordenados = rdd_empleados.sortByKey()\n",
        "\n",
        "resultado = rdd_ordenados.collect()\n",
        "print(\"Empleados ordenados por salario:\")\n",
        "for salario, empleado in resultado:\n",
        "    print(f\"{empleado}: {salario}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2WfxWDSxN9K",
        "outputId": "a2a6900c-6d77-4ba3-f05c-4d189d4503a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empleados ordenados por salario:\n",
            "Juan: 45000\n",
            "Ana: 48000\n",
            "Marta: 52000\n",
            "Carlos: 54000\n",
            "Sofía: 58000\n",
            "Luis: 60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Join:"
      ],
      "metadata": {
        "id": "pbW25aM1xjRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "empleados = [\n",
        "    (1, \"Ana\"),\n",
        "    (2, \"Carlos\"),\n",
        "    (3, \"Luis\"),\n",
        "    (4, \"Marta\"),\n",
        "    (5, \"Sofía\")\n",
        "]\n",
        "\n",
        "departamentos = [\n",
        "    (1, \"Ventas\", 48000),\n",
        "    (2, \"Marketing\", 54000),\n",
        "    (3, \"Finanzas\", 60000),\n",
        "    (4, \"Ventas\", 52000),\n",
        "    (5, \"Finanzas\", 58000)\n",
        "]\n",
        "\n",
        "rdd_empleados = sc.parallelize(empleados)\n",
        "rdd_departamentos = sc.parallelize(departamentos)\n",
        "\n",
        "rdd_empleados_departamentos = rdd_empleados.join(rdd_departamentos)\n",
        "\n",
        "resultado = rdd_empleados_departamentos.collect()\n",
        "print(\"Detalles completos de empleados y sus departamentos:\")\n",
        "for id_empleado, info in resultado:\n",
        "    nombre = info[0]\n",
        "    departamento = info[1][0]\n",
        "    salario = info[1][1]\n",
        "    print(f\"ID: {id_empleado}, Nombre: {nombre}, Departamento: {departamento}, Salario Anual: ${salario}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXyKUoEwxlSG",
        "outputId": "2d2ac835-0edd-4287-fff3-5dfd28712304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detalles completos de empleados y sus departamentos:\n",
            "ID: 4, Nombre: Marta, Departamento: V, Salario Anual: $e\n",
            "ID: 1, Nombre: Ana, Departamento: V, Salario Anual: $e\n",
            "ID: 5, Nombre: Sofía, Departamento: F, Salario Anual: $i\n",
            "ID: 2, Nombre: Carlos, Departamento: M, Salario Anual: $a\n",
            "ID: 3, Nombre: Luis, Departamento: F, Salario Anual: $i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CoGroup:"
      ],
      "metadata": {
        "id": "-N4rGIA4yNg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "empleados = [\n",
        "    (1, \"Ana\"),\n",
        "    (2, \"Carlos\"),\n",
        "    (3, \"Luis\"),\n",
        "    (4, \"Marta\"),\n",
        "    (5, \"Sofía\")\n",
        "]\n",
        "\n",
        "proyectos = [\n",
        "    (1, \"Proyecto A\"),\n",
        "    (1, \"Proyecto B\"),\n",
        "    (2, \"Proyecto A\"),\n",
        "    (3, \"Proyecto C\"),\n",
        "    (4, \"Proyecto A\"),\n",
        "    (4, \"Proyecto B\"),\n",
        "    (5, \"Proyecto D\")\n",
        "]\n",
        "\n",
        "horas_proyectos = [\n",
        "    (1, 20),\n",
        "    (1, 15),\n",
        "    (2, 30),\n",
        "    (3, 40),\n",
        "    (4, 25),\n",
        "    (4, 10),\n",
        "    (5, 35)\n",
        "]\n",
        "\n",
        "rdd_empleados = sc.parallelize(empleados)\n",
        "rdd_proyectos = sc.parallelize(proyectos)\n",
        "rdd_horas = sc.parallelize(horas_proyectos)\n",
        "\n",
        "rdd_proyectos_kv = rdd_proyectos.map(lambda x: (x[0], x[1]))\n",
        "rdd_horas_kv = rdd_horas.map(lambda x: (x[0], x[1]))\n",
        "\n",
        "rdd_empleados_proyectos_horas = rdd_proyectos_kv.cogroup(rdd_horas_kv)\n",
        "\n",
        "rdd_resultado = rdd_empleados.join(rdd_empleados_proyectos_horas)\n",
        "\n",
        "resultado = rdd_resultado.collect()\n",
        "print(\"Detalles de proyectos y horas dedicadas por cada empleado:\")\n",
        "for id_empleado, (nombre, (proyectos, horas)) in resultado:\n",
        "    proyectos = list(proyectos)\n",
        "    horas = list(horas)\n",
        "    print(f\"ID: {id_empleado}, Nombre: {nombre}, Proyectos: {proyectos}, Horas dedicadas: {horas}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLm6Nsg-yQDm",
        "outputId": "dadcd5a0-379b-47a4-8223-0e75f97fa10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detalles de proyectos y horas dedicadas por cada empleado:\n",
            "ID: 1, Nombre: Ana, Proyectos: ['Proyecto A', 'Proyecto B'], Horas dedicadas: [20, 15]\n",
            "ID: 2, Nombre: Carlos, Proyectos: ['Proyecto A'], Horas dedicadas: [30]\n",
            "ID: 3, Nombre: Luis, Proyectos: ['Proyecto C'], Horas dedicadas: [40]\n",
            "ID: 4, Nombre: Marta, Proyectos: ['Proyecto A', 'Proyecto B'], Horas dedicadas: [25, 10]\n",
            "ID: 5, Nombre: Sofía, Proyectos: ['Proyecto D'], Horas dedicadas: [35]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coalesce:"
      ],
      "metadata": {
        "id": "5GfSFJsqystm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "estudiantes = [\n",
        "    (\"Carlos\", \"Matemáticas\", 85),\n",
        "    (\"Ana\", \"Historia\", 78),\n",
        "    (\"Luis\", \"Matemáticas\", 92),\n",
        "    (\"Marta\", \"Ciencia\", 88),\n",
        "    (\"Sofía\", \"Historia\", 91),\n",
        "    (\"Juan\", \"Ciencia\", 95),\n",
        "    (\"Clara\", \"Matemáticas\", 74),\n",
        "    (\"Diego\", \"Historia\", 82),\n",
        "    (\"Laura\", \"Ciencia\", 80)\n",
        "]\n",
        "\n",
        "rdd_estudiantes = sc.parallelize(estudiantes, 4)\n",
        "print(\"Número de particiones originales:\", rdd_estudiantes.getNumPartitions())\n",
        "\n",
        "rdd_coalescido = rdd_estudiantes.coalesce(2)\n",
        "print(\"Número de particiones después de coalesce:\", rdd_coalescido.getNumPartitions())\n",
        "\n",
        "resultado = rdd_coalescido.collect()\n",
        "print(\"\\nDatos de los estudiantes:\")\n",
        "for estudiante in resultado:\n",
        "    print(estudiante)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzW7iorayxDw",
        "outputId": "464f5b23-f26a-4b4b-d708-6f653d7e0e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de particiones originales: 4\n",
            "Número de particiones después de coalesce: 2\n",
            "\n",
            "Datos de los estudiantes:\n",
            "('Carlos', 'Matemáticas', 85)\n",
            "('Ana', 'Historia', 78)\n",
            "('Luis', 'Matemáticas', 92)\n",
            "('Marta', 'Ciencia', 88)\n",
            "('Sofía', 'Historia', 91)\n",
            "('Juan', 'Ciencia', 95)\n",
            "('Clara', 'Matemáticas', 74)\n",
            "('Diego', 'Historia', 82)\n",
            "('Laura', 'Ciencia', 80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "estudiantes = [\n",
        "    (\"Carlos\", \"Matemáticas\", 85),\n",
        "    (\"Ana\", \"Historia\", 78),\n",
        "    (\"Luis\", \"Matemáticas\", 92),\n",
        "    (\"Marta\", \"Ciencia\", 88),\n",
        "    (\"Sofía\", \"Historia\", 91),\n",
        "    (\"Juan\", \"Ciencia\", 95),\n",
        "    (\"Clara\", \"Matemáticas\", 74),\n",
        "    (\"Diego\", \"Historia\", 82),\n",
        "    (\"Laura\", \"Ciencia\", 80)\n",
        "]\n",
        "\n",
        "rdd_estudiantes = sc.parallelize(estudiantes)\n",
        "max_calificacion = rdd_estudiantes.map(lambda x: x[2]).reduce(lambda a, b: a if a > b else b)\n",
        "print(\"La calificación más alta es:\", max_calificacion)\n",
        "# REDUCE (FUNC)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rajBbityFnti",
        "outputId": "0dd072d3-40e0-435d-8150-0bd7bbfa8563"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La calificación más alta es: 95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "estudiantes = [\n",
        "    (\"Carlos\", \"Matemáticas\", 85),\n",
        "    (\"Ana\", \"Historia\", 78),\n",
        "    (\"Luis\", \"Matemáticas\", 92),\n",
        "    (\"Marta\", \"Ciencia\", 88),\n",
        "    (\"Sofía\", \"Historia\", 91),\n",
        "    (\"Juan\", \"Ciencia\", 95),\n",
        "    (\"Clara\", \"Matemáticas\", 74),\n",
        "    (\"Diego\", \"Historia\", 82),\n",
        "    (\"Laura\", \"Ciencia\", 80)\n",
        "]\n",
        "\n",
        "rdd_estudiantes = sc.parallelize(estudiantes)\n",
        "# COLLECT (FUNC)\n",
        "resultado = rdd_estudiantes.collect()\n",
        "print(\"Datos de los estudiantes:\")\n",
        "for estudiante in resultado:\n",
        "    print(estudiante)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEg3n1k-F-l6",
        "outputId": "05799dcb-697d-4459-fff2-28b4c73aa6c4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos de los estudiantes:\n",
            "('Carlos', 'Matemáticas', 85)\n",
            "('Ana', 'Historia', 78)\n",
            "('Luis', 'Matemáticas', 92)\n",
            "('Marta', 'Ciencia', 88)\n",
            "('Sofía', 'Historia', 91)\n",
            "('Juan', 'Ciencia', 95)\n",
            "('Clara', 'Matemáticas', 74)\n",
            "('Diego', 'Historia', 82)\n",
            "('Laura', 'Ciencia', 80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "estudiantes = [\n",
        "    (\"Carlos\", \"Matemáticas\", 85),\n",
        "    (\"Ana\", \"Historia\", 78),\n",
        "    (\"Luis\", \"Matemáticas\", 92),\n",
        "    (\"Marta\", \"Ciencia\", 88),\n",
        "    (\"Sofía\", \"Historia\", 91),\n",
        "    (\"Juan\", \"Ciencia\", 95),\n",
        "    (\"Clara\", \"Matemáticas\", 74),\n",
        "    (\"Diego\", \"Historia\", 82),\n",
        "    (\"Laura\", \"Ciencia\", 80)\n",
        "]\n",
        "\n",
        "rdd_estudiantes = sc.parallelize(estudiantes)\n",
        "# Cuenta el nUmero de elementos en el RDD count (func)\n",
        "num_estudiantes = rdd_estudiantes.count()\n",
        "print(\"Número de estudiantes:\", num_estudiantes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7b3w5tOGmSV",
        "outputId": "697b07f4-d9d6-49b5-b59c-9bdb032db8a5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de estudiantes: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "estudiantes = [\n",
        "    (\"Carlos\", \"Matemáticas\", 85),\n",
        "    (\"Ana\", \"Historia\", 78),\n",
        "    (\"Luis\", \"Matemáticas\", 92),\n",
        "    (\"Marta\", \"Ciencia\", 88),\n",
        "    (\"Sofía\", \"Historia\", 91),\n",
        "    (\"Juan\", \"Ciencia\", 95),\n",
        "    (\"Clara\", \"Matemáticas\", 74),\n",
        "    (\"Diego\", \"Historia\", 82),\n",
        "    (\"Laura\", \"Ciencia\", 80)\n",
        "]\n",
        "\n",
        "rdd_estudiantes = sc.parallelize(estudiantes)\n",
        "# Obtiene el primer elemento del RDD FIRST (FUNC)\n",
        "primer_estudiante = rdd_estudiantes.first()\n",
        "print(\"Primer estudiante:\", primer_estudiante)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFgPMxdJHItu",
        "outputId": "5d74296a-44ea-4ba4-d181-78a73a659cd5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primer estudiante: ('Carlos', 'Matemáticas', 85)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "estudiantes = [\n",
        "    (\"Carlos\", \"Matemáticas\", 85),\n",
        "    (\"Ana\", \"Historia\", 78),\n",
        "    (\"Luis\", \"Matemáticas\", 92),\n",
        "    (\"Marta\", \"Ciencia\", 88),\n",
        "    (\"Sofía\", \"Historia\", 91),\n",
        "    (\"Juan\", \"Ciencia\", 95),\n",
        "    (\"Clara\", \"Matemáticas\", 74),\n",
        "    (\"Diego\", \"Historia\", 82),\n",
        "    (\"Laura\", \"Ciencia\", 80)\n",
        "]\n",
        "\n",
        "rdd_estudiantes = sc.parallelize(estudiantes)\n",
        "# Toma los primeros 3 elementos del RDD TAKE (FUNC)\n",
        "primeros_tres = rdd_estudiantes.take(3)\n",
        "print(\"Primeros tres estudiantes:\")\n",
        "for estudiante in primeros_tres:\n",
        "    print(estudiante)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aLuh4lIHUle",
        "outputId": "18b41267-ed08-46e3-e500-5788fa235ca7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeros tres estudiantes:\n",
            "('Carlos', 'Matemáticas', 85)\n",
            "('Ana', 'Historia', 78)\n",
            "('Luis', 'Matemáticas', 92)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "estudiantes = [\n",
        "    (\"Carlos\", \"Matemáticas\", 85),\n",
        "    (\"Ana\", \"Historia\", 78),\n",
        "    (\"Luis\", \"Matemáticas\", 92),\n",
        "    (\"Marta\", \"Ciencia\", 88),\n",
        "    (\"Sofía\", \"Historia\", 91),\n",
        "    (\"Juan\", \"Ciencia\", 95),\n",
        "    (\"Clara\", \"Matemáticas\", 74),\n",
        "    (\"Diego\", \"Historia\", 82),\n",
        "    (\"Laura\", \"Ciencia\", 80)\n",
        "]\n",
        "\n",
        "rdd_estudiantes = sc.parallelize(estudiantes)\n",
        "# guardamos el contenido del RDD en un archivo de texto saveAsTextFile\n",
        "rdd_estudiantes.saveAsTextFile(\"estudiantes_output\")\n",
        "\n",
        "print(\"Datos guardados en la carpeta 'estudiantes_output'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfNSujT6Hg3T",
        "outputId": "0e06017f-a0b5-476b-bbcf-9121f2600fa0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos guardados en la carpeta 'estudiantes_output'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "estudiantes = [\n",
        "    (\"Carlos\", \"Matemáticas\", 85),\n",
        "    (\"Ana\", \"Historia\", 78),\n",
        "    (\"Luis\", \"Matemáticas\", 92),\n",
        "    (\"Marta\", \"Ciencia\", 88),\n",
        "    (\"Sofía\", \"Historia\", 91),\n",
        "    (\"Juan\", \"Ciencia\", 95),\n",
        "    (\"Clara\", \"Matemáticas\", 74),\n",
        "    (\"Diego\", \"Historia\", 82),\n",
        "    (\"Laura\", \"Ciencia\", 80)\n",
        "]\n",
        "\n",
        "rdd_estudiantes = sc.parallelize(estudiantes)\n",
        "# Encuentra el estudiante con la calificación más baja MAX, MIN (FUNC)\n",
        "\n",
        "estudiante_max = rdd_estudiantes.max(key=lambda x: x[2])\n",
        "\n",
        "\n",
        "estudiante_min = rdd_estudiantes.min(key=lambda x: x[2])\n",
        "\n",
        "print(\"Estudiante con la calificación más alta:\", estudiante_max)\n",
        "print(\"Estudiante con la calificación más baja:\", estudiante_min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhO78UAgILAO",
        "outputId": "eb308f34-2a0e-42c5-8938-7c83968a6ddb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estudiante con la calificación más alta: ('Juan', 'Ciencia', 95)\n",
            "Estudiante con la calificación más baja: ('Clara', 'Matemáticas', 74)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "estudiantes = [\n",
        "    (\"Carlos\", \"Matemáticas\", 85),\n",
        "    (\"Ana\", \"Historia\", 78),\n",
        "    (\"Luis\", \"Matemáticas\", 92),\n",
        "    (\"Marta\", \"Ciencia\", 88),\n",
        "    (\"Sofía\", \"Historia\", 91),\n",
        "    (\"Juan\", \"Ciencia\", 95),\n",
        "    (\"Clara\", \"Matemáticas\", 74),\n",
        "    (\"Diego\", \"Historia\", 82),\n",
        "    (\"Laura\", \"Ciencia\", 80)\n",
        "]\n",
        "\n",
        "rdd_estudiantes = sc.parallelize(estudiantes)\n",
        "# se crea un RDD clave-valor donde la clave es la materia y el valor es 1\n",
        "rdd_materias = rdd_estudiantes.map(lambda x: (x[1], 1))\n",
        "\n",
        "# usamos countByKey para contar cuántos estudiantes hay en cada materia\n",
        "conteo_por_materia = rdd_materias.countByKey()\n",
        "\n",
        "print(\"Número de estudiantes por materia:\")\n",
        "for materia, conteo in conteo_por_materia.items():\n",
        "    print(materia, \":\", conteo)\n",
        "# countByKey"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onPBmawRIyOY",
        "outputId": "787ca132-9080-4971-c0f5-0dde0aaab247"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de estudiantes por materia:\n",
            "Matemáticas : 3\n",
            "Historia : 3\n",
            "Ciencia : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "estudiantes = [\n",
        "    (\"Carlos\", \"Matemáticas\", 85),\n",
        "    (\"Ana\", \"Historia\", 78),\n",
        "    (\"Luis\", \"Matemáticas\", 92),\n",
        "    (\"Marta\", \"Ciencia\", 88),\n",
        "    (\"Sofía\", \"Historia\", 91),\n",
        "    (\"Juan\", \"Ciencia\", 95),\n",
        "    (\"Clara\", \"Matemáticas\", 74),\n",
        "    (\"Diego\", \"Historia\", 82),\n",
        "    (\"Laura\", \"Ciencia\", 80)\n",
        "]\n",
        "\n",
        "rdd_estudiantes = sc.parallelize(estudiantes)\n",
        "# USAMOS foreach para imprimir cada estudiante con un formato personalizado\n",
        "\n",
        "rdd_estudiantes.foreach(lambda x: print(f\"Estudiante: {x[0]}, Materia: {x[1]}, Calificación: {x[2]}\"))\n",
        "\n",
        "resultado = rdd_estudiantes.collect()\n",
        "# FOREACH (FUNC)"
      ],
      "metadata": {
        "id": "A9EkMV-EKBL7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git\n",
        "\n",
        "!git config --global user.name \"raf4el12\"\n",
        "!git config --global user.email \"gomerorafael121@gmail.com\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQybC-9TLpKO",
        "outputId": "838352c0-1783-4fa5-fafd-7a924ad1e7fc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clona tu repositorio (reemplaza la URL por la de tu repositorio)\n",
        "!git clone https://github.com/raf4el12/queries_rdd.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1Fm6G_7MnKV",
        "outputId": "572a0ca4-7e38-4150-9909-12589b1077df"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'queries_rdd'...\n",
            "warning: You appear to have cloned an empty repository.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcI1MDc7M3KJ",
        "outputId": "b24bf0ba-b9db-40b4-cff1-e2bb34f78782"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"consultas de PySpark desde Colab\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw6Tgku_M-is",
        "outputId": "6be4847a-5007-491c-d4b5-098c973fbfea"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWddE-HGNFO0",
        "outputId": "a6128580-d0bb-4bb0-9108-c4a002cf7a3c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    }
  ]
}